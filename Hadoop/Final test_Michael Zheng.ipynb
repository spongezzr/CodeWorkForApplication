{"cells":[{"cell_type":"markdown","source":["### Final Test\n\nOur examples of finding the lowest-rated movies were polluted with movies rated by one or two people.\n\nYOUR GOAL: Modify both Demo Spark 1 and Demo Spark 2 scripts to only consider movies with at least ten ratings. \n\nHINTS:\n* RDD's have a filter function you can use.\n\n * It takes a function as a parameter, which accepts the entire key/value pair. \n * So if you are calling filter() on an RDD that contains (movie_id, (sumOfRatings, totalRatings)), a lambda function that takes in \"x\" would refer to totalRatings as x[1][1]. x[1] gives us the \"value\" of (sumOfRatings, totalRatings) and x[1][1] pulls out totalRatings. \n * This function should be an expression that returns True if the row should be kept, or False if it should be discarded. \n\n* DataFrames also have a filter() function. \n * It is easier, you just pass in a string expression for what you want to filter on.\n * For example: df.filter(\"count > 10\") would only pass through rows where the \"count\" column is greater than 10."],"metadata":{}},{"cell_type":"markdown","source":["If you havent uploaded u.data and u.item, you can follow the below steps:\n\nIn Databricks, you can easily create a table from a data file (e.g. csv). \n\nGo to the Data section, click on the default database and then click on the + (plus) sign at the top to create a new table.\n\n![table1](https://s3-us-west-1.amazonaws.com/julienheck/hadoop/9_final_test/databricks+-+create+table.png)\n\nFor the Data source, select \"Upload file\" and click on \"Drop file or click here to upload\". Browse to the file you want to upload.\n\n![table2](https://s3-us-west-1.amazonaws.com/julienheck/hadoop/9_final_test/databricks+-+create+table+2.png)\n\nOnce the file has been uploaded, click on \"Create Table with UI\".\n\n![table3](https://s3-us-west-1.amazonaws.com/julienheck/hadoop/9_final_test/databricks+-+create+table+3.png)\n\nSelect a running Cluster and click on \"Preview Table\".\n\nFinally, you should be able to specify the table attributes, such as the table and the columns name and the data type of each column.\n\n![table4](https://s3-us-west-1.amazonaws.com/julienheck/hadoop/9_final_test/databricks+-+create+table+4.png)\n\n\n\nFor `u.item` file, name the first 3 colums as follow: `movie_id`, `title`, `release_date`. Name the table `u_item`.\nData type is `string` by default, update it to `int` for `movie_id`.\n\nFor `u.data` file, name the colums as follow: `user_id`, `movie_id`, `rating`, `timestamp`. Name the table `u_data`.\nUpdate data type to `int` for `user_id`, `movie_id` and `rating`.\n\nClick on \"Create Table\" once you are done. You can now access the table in Databricks using Spark SQL commands."],"metadata":{}},{"cell_type":"code","source":["#List the path of the u.item and u.data\n\ndisplay(dbutils.fs.ls(\"/FileStore/tables/\"))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["### Code for your script with RDD ###\n\nrawItem = sc.textFile(\"dbfs:/FileStore/tables/u.item\")\nmovieList = rawItem.map(lambda line: line.split(\"|\")).map(lambda x: (int(x[0]), x[1]))\n\nrawData = sc.textFile(\"dbfs:/FileStore/tables/u.data\")\nmovieRatings = rawData.map(lambda line: line.split(\"\\t\")).map(lambda x: (int(x[1]), (int(x[2]), 1.0)))\n\n### Write your steps below ###\n\nratingTotalsAndCount = movieRatings.reduceByKey(lambda movie1, movie2 : (movie1[0]+movie2[0], movie1[1]+movie2[1]))\naverageRatings = ratingTotalsAndCount.filter(lambda x:x[1][1] > 10).mapValues(lambda totalAndCount: totalAndCount[0] / totalAndCount[1])\nsortedMovies = averageRatings.sortBy(lambda x: x[1])\njoinRDD = averageRatings.join(movieList)\nlistSorted = joinRDD.map(lambda x: (x[1]))\nresults = listSorted.sortBy(lambda x: x[0])\nresults.collect()\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["### Code for your script with DataFrames ###\n\nitemDF = sqlContext.sql(\"SELECT movie_id, title FROM u_item\")\ndataDF = sqlContext.read.format(\"csv\").options(inferschema='true', delimiter=\"\\t\").load(\"dbfs:/FileStore/tables/u.data\")\n\n### Write your steps below ###\n\ndataDF = sqlContext.sql(\"SELECT user_id, movie_id, rating FROM u_data\")\nmovieDF = dataDF.select(\"movie_id\", \"rating\").cache()\naverageRatings = movieDF.groupBy(\"movie_id\").avg(\"rating\")\ncounts = movieDF.groupBy(\"movie_id\").count() \naverageAndCounts = counts.join(averageRatings, \"movie_id\").join(itemDF, \"movie_id\")\nresult = averageAndCounts.filter('count > 10')\ntopTen = result.orderBy(\"avg(rating)\").select(\"title\", \"avg(rating)\", \"count\").show(10)\n\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Once you are done, download the notebook (click on File -> Export -> IPython notebook) and submit it through Canvas."],"metadata":{}}],"metadata":{"name":"Assignment Spark","notebookId":3547198959284155},"nbformat":4,"nbformat_minor":0}
